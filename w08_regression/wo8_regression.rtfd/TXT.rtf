{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Oblique;\f2\fnil\fcharset0 HelveticaNeue;
\f3\fswiss\fcharset0 Helvetica-Bold;\f4\froman\fcharset0 Times-Roman;\f5\fnil\fcharset0 STIXGeneral-Regular;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;\red255\green255\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0\c87059;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;
}
\margl1440\margr1440\vieww22620\viewh6720\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 1.Define an objective function,\
examples, ordinary least squares, Weight least squares\
\
\
1. Ordinary Least Squares,\
using a linear model:\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2023-04-30 at 9.50.44 PM.png \width1980 \height480 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
least square would be {{\NeXTGraphic Screen Shot 2023-04-30 at 9.51.43 PM.png \width6120 \height1080 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
Try to get that number from the equation, the lowest you can (so you an optimizer)\
for OLS, (ordinary least squares, RSS (residual sum of squaress) is the objective function.\
\
\
Anothe way to get to the same linear model\

\f1\i maximum likelihood estimation\

\f0\i0 for a linear model, \
\
{{\NeXTGraphic Screen Shot 2023-04-30 at 9.57.53 PM.png \width3800 \height520 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
2.\
{{\NeXTGraphic Screen Shot 2023-04-30 at 9.58.15 PM.png \width7000 \height1480 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\

\f2\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\
For Weighted Least Squares,
\f0\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {{\NeXTGraphic Screen Shot 2023-04-30 at 10.02.45 PM.png \width3840 \height520 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2023-04-30 at 10.02.56 PM.png \width7220 \height1400 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\
Least Squares with maximum likelihood\
\
\

\f3\b def nll(params, xs, ys, sigmas):\
    m, b = params\
    y_pred = m * xs + b\
    ll_terms = stats.norm.logpdf(ys - y_pred, loc=0, scale=sigmas)\
    return -ll_terms.sum()
\f0\b0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
code to run the optimizer\
\

\f3\b guess = np.array([0, 0]) # guess for m and b\
\
sigma_y = 1 # assume equal errors for all points\
\
minimization_ols = optimize.minimize(nll, guess, (data["conc"], data["A260"], sigma_y))\
m = minimization_ols.x[0]\
b = minimization_ols.x[1]\
print(f"OLS with maximum likelihood: y = \{m:.3f\}x + \{b:.2f\}")\
# calculate the y values predicted by our linear model\
predicted_ols = m * data["conc"] + b\
\

\f0\b0 output = 
\f4 \cf3 \cb4 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 OLS with maximum likelihood: y = 0.009x + 1.09\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3\b \cf0 \cb1 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \

\f0\b0 Using regular mean means, \
\
5/1/23\
Moriarity is using an Anayltical approach, that turns the harmonic regression into a linear regression\
equation.  The problem with his approach, is that is doesn't take into account the different sigmas of the eight\
different experients. your goals is to solve it Numerically.\
\
1. Try to run Moriarity script (\
1.b check it out \
\
now it's your turn\
2. Create the function  ("nll")\
probably something like\
\
\
\
def nll(params, ts, ys, sigmas):\
    a,b, phi = params\
    y_pred = b + a sin(2\uc0\u960 
\f5 \uc0\u55349 \u56420 
\f0 (ts+
\f5 phi
\f0 ))\
    ll_terms = stats.norm.logpdf(ys - y_pred, loc=0, scale=sigmas)\
    return -ll_terms.sum()\
\
Seperate out the sigmas from the ts data\
\
create a guess for the optimizier ,\
example of an optimizier \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3\b \cf0 optimize.minimize(nll, guess, (data["conc"], data["A260"], sigma_y))\
\
\
compare the solutions\
loop through every \
use an optimizer for every gene to get column like the output of Moriarity.\
\
loop through nll for each gene and sum them using Morarity Data,\
so this would look something like\
for g in gene:\
	params equal a,b and theta provided in Morairty numpy arrays, ts from the ts , ys from the numpy arrays and a sigma of ? (maybe be 1)}