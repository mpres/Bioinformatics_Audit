{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "69e848d2-3bbf-49bf-9288-c43567d93cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "import scipy.special as special\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20b049a2-7449-4ebf-9445-048b92182c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mpresley84/Desktop/w09_EM'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "#os.chdir('w09_EM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eafe5e67-d92c-4f01-9621-c43b92fa91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate Simulator\n",
    "\n",
    "# 2. Calculate the log likehood if the model and the parameters were known\n",
    "#    a. explain the steps of your calculation\n",
    "#    b. use your LL for the simulated data for your own parameter values\n",
    "#    c. compare Lestrade data to your actual data for your simualtor\n",
    "#    d. compare LL of Lestrade to your own of the true postitive control (yours should be better)\n",
    "\n",
    "# 3. Estimate isoform abundances by EM\n",
    "#    a. estimates the unkown isoform abundance ùùâ for each isoform (Ar1..Arc10). given read couts r and the sturucture of the Arc locus including the lengths Li, using expectation maximation\n",
    "#.   b. apply your Emto lestrade supplementary data\n",
    "#.   c. what are the most abundant transcripts\n",
    "#.   d. what are the least abundant transcripts\n",
    "#.   e. Explain why lestrade's data is wrong,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70e5b88c-7053-4c68-9963-f731ea28e693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arc1': 0.01180967282153745,\n",
       " 'Arc2': 0.02732962316129933,\n",
       " 'Arc3': 0.027995994879347962,\n",
       " 'Arc4': 0.2838608278561838,\n",
       " 'Arc5': 0.10066596297998968,\n",
       " 'Arc6': 0.367245947991555,\n",
       " 'Acr7': 0.013413781539448288,\n",
       " 'Acr8': 0.13627917267556278,\n",
       " 'Acr9': 0.024431646613959697,\n",
       " 'Arc10': 0.00696736948111594}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mpresley 5/10/23\n",
    "# 1. Simulator\n",
    "\n",
    "#a.import model, as Panda table\n",
    "#b. Create a distribution of Direction that will create 10 values that sum up to one for the simualtor\n",
    "#theta = np.random.dirichlet(np.ones(10))\n",
    "Seq = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\"]\n",
    "Arc_locs = {'Arc1':'ABCD','Arc2':'BC','Arc3':'CDE','Arc4':'DEFG','Arc5':'EFGH','Arc6':'EGH','Acr7':'GH','Acr8':'HI','Acr9':'IJA','Arc10':'JAB'}\n",
    "Arc_keys = list(Arc_locs.keys())\n",
    "#taus = {Arc_keys[i]: theta[i] for i in range(len(Arc_locs.keys()))}\n",
    "os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3297015f-6951-44b3-962b-3c125d98822c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JAB'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#list(\"ABC\")\n",
    "np.random.choice(list(Arc_locs.values()))\n",
    "\n",
    "\n",
    "#np.random.choice([\"ABC\".split()])\n",
    "#    res.append(np.random.choice([\"A\",\"B\",\"C\",\"D\",\"E\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1e190226-be7c-42de-adee-b566b4188808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateRead(N_abunds,isoForms):\n",
    "    loc = np.random.choice(isoForms,1,p=N_abunds)\n",
    "    read = np.random.choice(list(loc[0]))\n",
    "    return read\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e577db-8fb2-4c6c-8648-6b30aca4f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "GenerateRead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a3057b1e-45af-458c-b768-40a03e750497",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(100000):\n",
    "    res.append(GenerateRead(list(taus.values()) ,list(Arc_locs.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "28ae19c4-9e23-4e6c-842c-e5099ad9331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateModel(Taus,Arcs,N):\n",
    "    #get Lengths from Arcs\n",
    "    res = []\n",
    "    seq = ['A','B','C','D','E','F','G','H','I','J']\n",
    "    res_dict = {}\n",
    "    Arc_lens = [len(arc) for arc in Arc_locs.values()]\n",
    "    Ns = [Taus[i] * Arc_lens[i] for i in range(len(Taus))]\n",
    "    N_abunds = Ns/sum(Ns)\n",
    "    print(N_abunds)\n",
    "    print(Arc_locs.values())\n",
    "    for i in range(N):\n",
    "        res.append(GenerateRead(N_abunds,list(Arc_locs.values())))\n",
    "    for s in seq:\n",
    "        result_dict[s] = res.count(s)\n",
    "    return result_dict\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "42be4140-07ad-4561-8180-467106d710b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01467353 0.01697854 0.02608878 0.3526973  0.12507754 0.34222753\n",
      " 0.00833332 0.08466349 0.02276725 0.00649272]\n",
      "dict_values(['ABCD', 'BC', 'CDE', 'DEFG', 'EFGH', 'EGH', 'GH', 'HI', 'IJA', 'JAB'])\n"
     ]
    }
   ],
   "source": [
    "#Mpresley generated on 5/15/23 \n",
    "simulated_Reads = GenerateModel(list(taus.values()),Arc_locs,1000000)\n",
    "\n",
    "\n",
    "#list(taus.values())\n",
    "#Arc_locs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "c89af005-85ee-421d-aeca-5635e210577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 13256, 'B': 14300, 'C': 21079, 'D': 100157, 'E': 243221, 'F': 119442, 'G': 237388, 'H': 192134, 'I': 49468, 'J': 9555}\n",
      "{'Arc1': 0.01180967282153745, 'Arc2': 0.02732962316129933, 'Arc3': 0.027995994879347962, 'Arc4': 0.2838608278561838, 'Arc5': 0.10066596297998968, 'Arc6': 0.367245947991555, 'Acr7': 0.013413781539448288, 'Acr8': 0.13627917267556278, 'Acr9': 0.024431646613959697, 'Arc10': 0.00696736948111594}\n"
     ]
    }
   ],
   "source": [
    "#MPresley 5/15/23\n",
    "print(simulated_Reads)\n",
    "print(taus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4e284f2a-1b7c-469a-9006-27b77df05adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 3, 4, 4, 3, 2, 2, 3, 3]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(arc) for arc in Arc_locs.values() ]\n",
    "#len(list(Arc_locs.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6acddc9e-883b-4ae2-a573-8e4cdd255070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1310\n",
      "1902\n",
      "2654\n",
      "8241\n",
      "22549\n",
      "9730\n",
      "22644\n",
      "22276\n",
      "7632\n",
      "1062\n",
      "{'A': 1310, 'B': 1902, 'C': 2654, 'D': 8241, 'E': 22549, 'F': 9730, 'G': 22644, 'H': 22276, 'I': 7632, 'J': 1062}\n"
     ]
    }
   ],
   "source": [
    "Seq\n",
    "result_dict = {}\n",
    "for s in Seq:\n",
    "    print(res.count(s))\n",
    "    result_dict[s] = res.count(s)\n",
    "print(result_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "70cbe700-bf94-475b-82df-ab8e7ab2597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mpresley 5/12/23, let's try to program the loglikehood script using the student's homework (needed a little bit of help here)\n",
    "def getSegmentProbDict (theta,Segments,Arcs):\n",
    "    #theta = nucleotide abundances, so for example [.3,.2,.3.,.1,.1] (should add up to one, but there is 10 of then and not 5 like in this example\n",
    "    #Segment will be a...j , up to 10\n",
    "    #Arcs is the Arc structure, will be some combination of the segments, so for exampe {ab,bc,bcd,cd} etc should also add up to 10.\n",
    "    \n",
    "    #first let's get the lengths of the Arcs\n",
    "    arc_lengths_dict = {}\n",
    "    theta_dict = {}\n",
    "    SegmentProbDict  = {}\n",
    "    #Make dictionaries for easy referencing\n",
    "    for a in Arcs:\n",
    "        arc_lengths_dict[a] = len(a)\n",
    "    \n",
    "    for i in range(len(theta)):\n",
    "        theta_dict[Arcs[i]] = theta[i]\n",
    "    \n",
    "    \n",
    "    #iterative through the lengths\n",
    "    for s in Segments:\n",
    "        s_list = []\n",
    "        for a in Arcs:\n",
    "            if s in a:\n",
    "                s_list.append(np.log(theta_dict[a] * 1/arc_lengths_dict[a]))\n",
    "\n",
    "        SegmentProbDict[s] = s_list\n",
    "    return SegmentProbDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2963dc78-91c7-4383-a31f-12cea629fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Segments = ['A','B','C','D','E','F','G','H','I','J']\n",
    "Arcs = list(Arc_locs.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "b77315ac-33be-4920-be47-4d9ec2976cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "segtest = getSegmentProbDict(theta,Segments,Arcs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a663602f-5bfd-49f9-a0d2-3ce41788241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSumProbArray(SegmentProbDict):\n",
    "    S_likehood = {}\n",
    "    for s in SegmentProbDict:\n",
    "        S_likehood[s] = special.logsumexp(SegmentProbDict[s])\n",
    "    return S_likehood\n",
    "#special.logsumexp(segtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "d12353b9-d6c7-4f75-aba2-6128b29270bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': -4.311101781852516,\n",
       " 'B': -3.966495755261106,\n",
       " 'C': -3.651613416280882,\n",
       " 'D': -2.4859116731042294,\n",
       " 'E': -1.4789404399974544,\n",
       " 'F': -2.3420361765064897,\n",
       " 'G': -1.4905270486774285,\n",
       " 'H': -1.5031505491087094,\n",
       " 'I': -2.573299028092897,\n",
       " 'J': -4.559591009778708}"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getSumProbArray(segtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "60471019-c494-4919-821b-e3376d7724ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalProbArray(S_likehood,read_counts):\n",
    "    S_likehood_total = []\n",
    "    for s in S_likehood:\n",
    "        S_likehood_total.append(S_likehood[s] * read_counts[s])\n",
    "    return sum(S_likehood_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "c593cd33-33bf-4c96-af5f-373057613f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLL(theta,Arcs,read_counts):\n",
    "    Segments = list(read_counts.keys())\n",
    "    segmentProbDict =  getSegmentProbDict(theta,Segments,Arcs)\n",
    "    S_likehood = getSumProbArray(segmentProbDict)\n",
    "    \n",
    "    return getTotalProbArray(S_likehood,read_counts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "6f8cc4c3-3b96-4c2a-b467-6d143bfba42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_counts = {'A':97512,'B':268771,'C':249157,'D':76643,'E':33977,'F':40729,'G':43351,'H':76299,'I':73029,'J':40532,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "0baf724e-633f-4ff0-9306-014043d2a5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3284494.2300386424"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getSegmentProbDict(theta,segments,arcs)\n",
    "getTotalProbArray(S_likehood,read_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "239b63f1-fbe6-4409-be5c-81995389460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testResults = getLL(theta,Arcs,read_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "fc3def2e-0839-432d-a273-0eade25525da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3284494.2300386424"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "af6e5043-f3ef-4997-9fcf-60644a332ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027995994879347962"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta[Segments.index(\"C\")]\n",
    "#Segments.index(\"C\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "e0a939fd-a28c-4392-925d-770fcab6a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Let's generate the EM function\n",
    "def Expectation(theta,read_counts,Arcs):\n",
    "    #read_counts should be a dictionar of counts\n",
    "    #theta should be an array that sums up to 1 \n",
    "    #Arcs should be an array of the Arc Transcripts ex [\"ABCD\",\"BC\"...etc]\n",
    "    \n",
    "    #first let's create the segmetns from the keys of the read_counts dictionary\n",
    "    Segments = list(read_counts.keys())\n",
    "    Arc_lengths = [len(a) for a in Arcs]\n",
    "      \n",
    "\n",
    "    #let's creata theta dictionary from the theta array and the Segments list\n",
    "    theta_dict = {}\n",
    "    for s in Segments:\n",
    "        theta_dict[s] = theta[Segments.index(s)]\n",
    "    prob_dict = {}\n",
    "    prob_singleRead_array = []\n",
    "    joint_prob = getSegmentProbDict(theta,Segments,Arcs)\n",
    "    sProbs = getSumProbArray(joint_prob)\n",
    "    for s in Segments:\n",
    "        prob_singleRead_array = []\n",
    "        for t in Arcs:\n",
    "            if s in t:\n",
    "                num = np.log(theta_dict[s] * (1/Arc_lengths[Arcs.index(t)]))\n",
    "                denum = sProbs[s]\n",
    "                prob_singleRead_array.append(num-denum)\n",
    "            else:\n",
    "                prob_singleRead_array.append(0)\n",
    "       # prob_singleRead_array = np.array(prob_singleRead_array)       \n",
    "#        prob_singleRead_array = prob_singleRead_array/prob_singleRead_array.sum()\n",
    "        prob_dict[s] = np.array(prob_singleRead_array)\n",
    "    #Now we have the probablity dictionary matrix of one read. posterior probablity matrix, lets multiple the reads\n",
    "    prob_dict_sums = {}\n",
    "    for s in prob_dict:\n",
    "        prob_dict_sums[s] = prob_dict[s] * read_counts[s]\n",
    "#print(prob_dict[s] * read_counts[s])\n",
    "    return prob_dict_sums\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "ce9c5129-089c-4fff-b0dd-11f3b2bad2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "testExpect = Expectation2(theta,read_counts,Arcs)\n",
    "#testarray = [5,3,5]\n",
    "#testarray2 = np.array([5,3,5])\n",
    "#print(testarray2*5)\n",
    "#testExpect['A']\n",
    "#arraytest = np.array([25,5,3,15,40])\n",
    "#arraytest/arraytest.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "a25fedd9-2c2e-417e-8052-bd5ced772227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.51402893  0.          0.          0.          0.          0.\n",
      "  0.          0.         -1.22634686 -1.22634686]\n",
      "[-1.5140289319347904, 0, 0, 0, 0, 0, 0, 0, -0.49938650149123287, -1.7540280381265347]\n"
     ]
    }
   ],
   "source": [
    "print(testExpect['A'])\n",
    "print(log_step1['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "d8b22548-b929-4bf5-864c-cdeef6f14884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.array(testarray) \n",
    "testexpect_pd = pd.DataFrame(testExpect)\n",
    "#testexpect_pd.count()\n",
    "len(testexpect_pd.index)\n",
    "#for i in range(10):\n",
    " #   print(i)\n",
    "#sum(testexpect_pd.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "8a5db766-7b46-4bf2-bae8-ce80af354431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mpresley 5/12/23, attached is the maxmize step\n",
    "def Maxmization(postprobs):\n",
    "    #postprobs is a dictionary\n",
    "    # The 1st step is to convert the postprobs dictionary into a panda dataframe\n",
    "    postprobs_df = pd.DataFrame(postprobs)\n",
    "    theta = []\n",
    "    # The 2nd step is to sum the columns for the thetas\n",
    "    for i in range(len(postprobs_df.index)):\n",
    "        theta.append(sum(postprobs_df.iloc[i]))\n",
    "    # 3rd we will normalize the theta array\n",
    "    normalized_theta = [t/sum(theta) for t in theta] \n",
    "    return normalized_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "id": "a0cd8376-c673-40b8-b535-4e6f63f89e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(theta,read_counts,Arcs,converge=None,maxrounds=100):\n",
    "    #rounds \n",
    "    old_ll = 0\n",
    "    for r in range(maxrounds):\n",
    "        counts = Expectation2(theta,read_counts,Arcs)\n",
    "        theta = Maxmization2(counts)\n",
    "        ll = getLL(theta,Arcs,read_counts)\n",
    "        delta = (ll - old_ll)\n",
    "        old_ll = ll\n",
    "    #    print('delta',delta)\n",
    "        if converge != None and abs(delta) < converge:\n",
    "      #      print('converge, delta', converge,delta)\n",
    "       #     print('ll, theta',ll,theta)\n",
    "            return(ll,theta)\n",
    "    return(ll,theta)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "de7e3454-2a4f-48fc-84bc-37560512ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunc(a,b=None):\n",
    "    if b == None:\n",
    "        return(a)\n",
    "    else:\n",
    "        return(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "b593ccf0-2c95-4677-ba63-a193a7eb0afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testFunc(\"hello\")\n",
    "abs(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "f04a3b5f-6bf8-4131-b7c4-fd5b628cd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunc(a,b):\n",
    "    return(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "id": "84af7a22-e44b-4bc6-8e5d-cb4a5fc6f197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round_b -2022308.9707417663\n",
      "round_c -2022239.2255290712\n"
     ]
    }
   ],
   "source": [
    "#result_10_round = EM(theta,read_counts,Arcs,None,10)\n",
    "result_100_round_b = EM(theta,read_counts,Arcs,1,100)\n",
    "result_100_round_c = EM(theta,read_counts,Arcs,None,10000)\n",
    "#result_1000_round = EM(theta,read_counts,Arcs,None,1000)\n",
    "print('round_b',result_100_round_b[0])\n",
    "print('round_c',result_100_round_c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "85c7ec23-9a6f-44e1-99c4-2c4e3ab8c07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABCD', 'BC', 'CDE', 'DEFG', 'EFGH', 'EGH', 'GH', 'HI', 'IJA', 'JAB']"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result_100_round_b\n",
    "Arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "id": "465bca64-0414-40eb-8337-4957dd537910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2022239.2255290712\n"
     ]
    }
   ],
   "source": [
    "print(result_100_round_c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "54734867-e0b1-47cc-9248-98be66b88e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2423044.0239493162,\n",
       " [0.21904943050965234,\n",
       "  0.04213141141136058,\n",
       "  0.14188395515547392,\n",
       "  0.23956671365801732,\n",
       "  0.17273699913628762,\n",
       "  0.06354567288622934,\n",
       "  0.032301275714605185,\n",
       "  0.03583963757454473,\n",
       "  0.014201801469384942,\n",
       "  0.038743102484443834])"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "e984ef31-5e7f-4437-a6ed-7f845146715d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2236414.7036050637,\n",
       " [0.32747509156516685,\n",
       "  0.0842940815677223,\n",
       "  0.04718782522175288,\n",
       "  0.010509815055131173,\n",
       "  0.022609784678229324,\n",
       "  0.015002958947175873,\n",
       "  0.008377551374874151,\n",
       "  0.043388716287826194,\n",
       "  0.20542472144719814,\n",
       "  0.23572945385492325])"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_10_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "fff799c2-50a2-4159-97d3-ed903fae4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_100_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "475b2d75-812a-48c6-87f6-af3592dbb4db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [532]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Parse the input file.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#   The first line is \"The <n> transcripts of the sand mouse Arc locus\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     line  \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m     30\u001b[0m     match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^The (\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+) transcripts\u001b[39m\u001b[38;5;124m'\u001b[39m, line)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "#using Lestrade code (this is for question 2 b. use Lestrade to process the your data (what your simulator made), b.ii compare it to the actuall thetas, use the Log-Likehood\n",
    "#what are the most abundant two transcripts, c. What are the least two abundant ones 4. what do you think of Lestrade conclusion\n",
    "\n",
    "#! /usr/bin/env python3\n",
    "\n",
    "# w09-naive.py\n",
    "#   The analysis done by Lestrade et al.\n",
    "#   Also serves as an example of parsing the input data.\n",
    "# \n",
    "# Usage:\n",
    "#   ./w09-naive.py <infile>\n",
    "#\n",
    "# Estimate the number of reads assigned to each isoform by a naive\n",
    "# method.  Assign each read to its cognate segment, trusting the\n",
    "# mapping and ignoring accuracy; then assign to isoform\n",
    "# uniformly. (i.e. if 3 isoforms share the same segment, assign 1/3\n",
    "# count to each isoform.)\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Parse the input file.\n",
    "#\n",
    "with open(sys.argv[1]) as f:\n",
    "    #   The first line is \"The <n> transcripts of the sand mouse Arc locus\"\n",
    "    line  = f.readline()\n",
    "    match = re.search(r'^The (\\d+) transcripts', line)\n",
    "    T     = int(match.group(1))\n",
    "\n",
    "    # The next T lines are \n",
    "    #   <Arcn>  <true_tau> <L> <structure>\n",
    "    # tau's may be present, or obscured (\"xxxxx\")\n",
    "    tau       = np.zeros(T)\n",
    "    L         = np.zeros(T).astype(int)\n",
    "    tau_known = True   # until we see otherwise\n",
    "    for i in range(T):\n",
    "        fields    = f.readline().split()\n",
    "        if fields[1] == \"xxxxx\":\n",
    "            tau_known = False\n",
    "        else:\n",
    "            tau[i] = float(fields[1])\n",
    "        L[i]      = int(fields[2])\n",
    "\n",
    "    # after a blank line,\n",
    "    # 'The <n> read sequences':\n",
    "    line  = f.readline()\n",
    "    line  = f.readline()\n",
    "    match = re.search(r'The (\\d+) read sequences', line)\n",
    "    N     = int(match.group(1))\n",
    "\n",
    "    # the next T lines are \n",
    "    #  <read a-j> <count>\n",
    "    r = np.zeros(T).astype(int)\n",
    "    for k in range(T):\n",
    "        fields = f.readline().split()\n",
    "        r[k]   = fields[1]\n",
    "\n",
    "\n",
    "S = T    # S = R = T : there are T transcripts (Arc1..Arc10), S segments (A..J), R reads (a..j)\n",
    "R = T\n",
    "Slabel   = list(string.ascii_uppercase)[:S]               # ['A'..'J']        : the upper case labels for Arc locus segments \n",
    "Tlabel   = [ \"Arc{}\".format(d) for d in range(1,T+1) ]    # ['Arc1'..'Arc10'] : the labels for Arc transcript isoforms\n",
    "Rlabel   = list(string.ascii_lowercase)[:T]               # ['a'..'j']        : lower case labels for reads\n",
    "\n",
    "\n",
    "# Count how often each segment A..J is used in the isoforms i\n",
    "# We'll use that to split observed read counts across the isoforms\n",
    "# that they might have come from.\n",
    "#\n",
    "segusage = np.zeros(S).astype(int)\n",
    "for i in range(T):\n",
    "    for j in range(i,i+L[i]): \n",
    "        segusage[j%S] += 1\n",
    "\n",
    "\n",
    "# Naive analysis:\n",
    "#\n",
    "c  = np.zeros(T)\n",
    "for i in range(T):\n",
    "    for k in range(i,i+L[i]):\n",
    "        c[i] += (1.0 / float(segusage[k%S])) * float(r[k%S])  # For each read k, assume read k-> segment j,\n",
    "                                                              # and assign 1/usage count to each transcript\n",
    "                                                              # that contains segment j.\n",
    "Z       = np.sum(c)\n",
    "est_nu  = np.divide(c, Z)       # nucleotide abundance\n",
    "\n",
    "print(c)\n",
    "\n",
    "est_tau = np.divide(est_nu, L)  # convert to TPM, transcript abundance\n",
    "est_tau = np.divide(est_tau, np.sum(est_tau))\n",
    "\n",
    "# Print a table of the resulting estimates for tau\n",
    "for i in range(T):\n",
    "    print (\"{0:10s}  {1:5.3f}\".format(Tlabel[i], est_tau[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "6b81ce46-bbfd-4b2b-a27d-cacabaecf05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{'A': 13256,\\n 'B': 14300,\\n 'C': 21079,\\n 'D': 100157,\\n 'E': 243221,\\n 'F': 119442,\\n 'G': 237388,\\n 'H': 192134,\\n 'I': 49468,\\n 'J': 9555} \""
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mpresley 5/15/23, out for w09-naive.py, from data of simulated_Reads \n",
    "'''\n",
    "[ 49597.33333333  11793.         121485.66666667 213620.33333333\n",
    " 228268.16666667 147194.5        107380.5         72767.5\n",
    "  33930.16666667  13962.83333333]\n",
    "\n",
    "\n",
    "Arc1        0.038\n",
    "Arc2        0.018\n",
    "Arc3        0.125\n",
    "Arc4        0.165\n",
    "Arc5        0.176\n",
    "Arc6        0.151\n",
    "Arc7        0.166\n",
    "Arc8        0.112\n",
    "Arc9        0.035\n",
    "Arc10       0.014\n",
    "\n",
    "\n",
    "#Mpresley 5/15/23, true arcs\n",
    "{'Arc1': 0.01180967282153745,\n",
    " 'Arc2': 0.02732962316129933,\n",
    " 'Arc3': 0.027995994879347962,\n",
    " 'Arc4': 0.2838608278561838,\n",
    " 'Arc5': 0.10066596297998968,\n",
    " 'Arc6': 0.367245947991555,\n",
    " 'Acr7': 0.013413781539448288,\n",
    " 'Acr8': 0.13627917267556278,\n",
    " 'Acr9': 0.024431646613959697,\n",
    " 'Arc10': 0.00696736948111594}\n",
    " \n",
    " true reads\n",
    "\n",
    "'''\n",
    "#taus\n",
    "# True Counts\n",
    "\"'A': 13256, 'B': 14300, 'C': 21079, 'D': 100157, 'E': 243221, 'F': 119442, 'G': 237388, 'H': 192134, 'I': 49468, 'J': 9555}\"\n",
    "'''\n",
    "{'A': 13256,\n",
    " 'B': 14300,\n",
    " 'C': 21079,\n",
    " 'D': 100157,\n",
    " 'E': 243221,\n",
    " 'F': 119442,\n",
    " 'G': 237388,\n",
    " 'H': 192134,\n",
    " 'I': 49468,\n",
    " 'J': 9555} '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "e14a961e-145b-4a8e-b3a6-f4ca7c3e6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "##testResults = getLL(theta,Arcs,read_counts)\n",
    "#geeting getLL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "b520dd1a-5b72-479a-9266-0a21bb9e13c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Arc1        0.038\n",
    "Arc2        0.018\n",
    "Arc3        0.125\n",
    "Arc4        0.165\n",
    "Arc5        0.176\n",
    "Arc6        0.151\n",
    "Arc7        0.166\n",
    "Arc8        0.112\n",
    "Arc9        0.035\n",
    "Arc10       0.014 '''\n",
    "Lestrade_theta = {'Arc1' : .038,\n",
    "                  'Arc2' : .018,\n",
    "                  'Arc3' : .125,\n",
    "                  'Arc4' : .165,\n",
    "                  'Arc5' : .176,\n",
    "                  'Arc6' : .151,\n",
    "                  'Arc7' : .166,\n",
    "                  'Arc8' : .112,\n",
    "                  'Arc9' : .035,\n",
    "                  'Arc10' : .014}\n",
    "\n",
    "#getLL(Lestrade_theta, Arcs, simulated_Reads )\n",
    "#simulated_Reads\n",
    "#read_counts\n",
    "#type(theta)\n",
    "#Lestrade_theta_array = np.array(list(Lestrade_theta.values()))\n",
    "#Lestrade_theta_array\n",
    "#getLL(Lestrade_theta_array,Arcs,simulated_Reads)\n",
    "# The ouput is -1928094.320718252, of the LL\n",
    "#looking at the true taus\n",
    "#getLL(taus,Arcs,simulated_Reads)\n",
    "#taus_array = np.array(list(taus.values()))\n",
    "\n",
    "#getLL(taus_array,Arcs,simulated_Reads) gave an output of -1892771.93797029,\n",
    "# this gives a difference of 35,323\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "06a6aba6-d524-4be1-a987-aca96fb9028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The thetas are  [0.3301361442294943, 0.08673428285972554, 0.053980871079123936, 0.013160703883100566, 0.023380865096103758, 0.0153264454492793, 0.008343305500636267, 0.04160639614917142, 0.19800192447969064, 0.22932906127367433]\n"
     ]
    }
   ],
   "source": [
    "# result_100_round_c this was used by getting a converge of .01 delta (pretty small amount)\n",
    "#The thetas for this are \n",
    "print(\"The thetas are \",result_100_round_c[1])\n",
    "results = list(result_100_round_c[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "09a5dd28-b523-4180-a95a-9aeba359e200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABCD': 0.3301361442294943,\n",
       " 'BC': 0.08673428285972554,\n",
       " 'CDE': 0.053980871079123936,\n",
       " 'DEFG': 0.013160703883100566,\n",
       " 'EFGH': 0.023380865096103758,\n",
       " 'EGH': 0.0153264454492793,\n",
       " 'GH': 0.008343305500636267,\n",
       " 'HI': 0.04160639614917142,\n",
       " 'IJA': 0.19800192447969064,\n",
       " 'JAB': 0.22932906127367433}"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Arcs\n",
    "for a in Arcs:\n",
    "    Arc_Results[a] = results[Arcs.index(a)]\n",
    "Arc_Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "3dfd8fb6-02d0-4933-934c-cfd91345ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The highest transcript are ABCD and JAB and account for about 50 percent of the expression levels\"\n",
    "# The lowest transcript are DEFG and GH,\n",
    "#BC and CDE are both expressed at pretty low levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "824b16d6-5b78-4d7e-bc7b-30d2d3815fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 97512,\n",
       " 'B': 268771,\n",
       " 'C': 249157,\n",
       " 'D': 76643,\n",
       " 'E': 33977,\n",
       " 'F': 40729,\n",
       " 'G': 43351,\n",
       " 'H': 76299,\n",
       " 'I': 73029,\n",
       " 'J': 40532}"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "id": "02d4cb34-1509-42a3-94f2-8efe0b36b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob_T(nu, transcriptome, S):\n",
    "    L = [len(t) for t in transcriptome]\n",
    "    logprob_Ts = {}\n",
    "    \n",
    "    joint_loglikelihoods = logjoint(nu, transcriptome, S)\n",
    "    S_loglikelihoods = logS(joint_loglikelihoods)\n",
    "        \n",
    "    for s in S:\n",
    "        logprob_Ts[s] = []\n",
    "        for t in range(0, len(transcriptome)):\n",
    "            if s in transcriptome[t]:\n",
    "                num = np.log((1 / L[t]) * nu[t])\n",
    "                denom = S_loglikelihoods[s]\n",
    "                logprob_Ts[s].append(num - denom)\n",
    "            else:\n",
    "                logprob_Ts[s].append(0)\n",
    "    \n",
    "    return logprob_Ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "a896cda6-aeaa-47cc-bb03-b2a43ea22a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_counts(logprob_Ts, reads, S):\n",
    "    counts = [0] * 10\n",
    "    \n",
    "    for s in logprob_Ts:\n",
    "        read_s = reads[s]\n",
    "        for t in range(0, len(logprob_Ts[s])):\n",
    "            if logprob_Ts[s][t] != 0:\n",
    "                counts[t] += read_s * np.exp(logprob_Ts[s][t])\n",
    "                \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "457aabf3-6fad-4bb1-b27d-5869fdb9ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nu(assigned_counts):\n",
    "    total = sum(assigned_counts)\n",
    "    return [c/total for c in assigned_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "062aa6be-f9e0-4552-8b2b-608e50458033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 calculate log P(S, T| v, L) for each combination of S and T\n",
    "def logjoint(nu, transcriptome, S):\n",
    "    L = [len(t) for t in transcriptome]\n",
    "    joint_likelihoods = {}\n",
    "    for s in S:\n",
    "        joint_likelihoods[s] = []\n",
    "        \n",
    "        # for each transcript t\n",
    "        for t in range(0, len(transcriptome)): \n",
    "            if s in transcriptome[t]:\n",
    "                # only consider if the segment is present in the transcript \n",
    "                log_likelihood = np.log((1 / L[t]) * nu[t])\n",
    "                joint_likelihoods[s].append(log_likelihood)\n",
    "    \n",
    "    return joint_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "30be6208-ba57-4c39-ac1b-c6f0fb7e0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 calculate log P(S | v, L) by marginalizing over T\n",
    "def logS(joint_loglikelihoods):\n",
    "    S_likelihoods = {}\n",
    "    for s in joint_loglikelihoods:\n",
    "        S_likelihoods[s] = special.logsumexp(joint_loglikelihoods[s])\n",
    "    \n",
    "    return S_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "d7a95561-19d3-4c69-b38c-2834f3c26d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logprob_T\n",
    "#Expectation(theta,read_counts,Arcs)\n",
    "#logprob_T(nu, transcriptome, S)\n",
    "#Segments = list(read_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "id": "e8bd9ab1-ca23-418b-81cf-ba60009c9ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denom -4.311101781852516\n",
      "denom -4.311101781852516\n",
      "denom -4.311101781852516\n",
      "denom -3.966495755261106\n",
      "denom -3.966495755261106\n",
      "denom -3.966495755261106\n",
      "denom -3.651613416280882\n",
      "denom -3.651613416280882\n",
      "denom -3.651613416280882\n",
      "denom -2.4859116731042294\n",
      "denom -2.4859116731042294\n",
      "denom -2.4859116731042294\n",
      "denom -1.4789404399974544\n",
      "denom -1.4789404399974544\n",
      "denom -1.4789404399974544\n",
      "denom -1.4789404399974544\n",
      "denom -2.3420361765064897\n",
      "denom -2.3420361765064897\n",
      "denom -1.4905270486774285\n",
      "denom -1.4905270486774285\n",
      "denom -1.4905270486774285\n",
      "denom -1.4905270486774285\n",
      "denom -1.5031505491087094\n",
      "denom -1.5031505491087094\n",
      "denom -1.5031505491087094\n",
      "denom -1.5031505491087094\n",
      "denom -2.573299028092897\n",
      "denom -2.573299028092897\n",
      "denom -4.559591009778708\n",
      "denom -4.559591009778708\n"
     ]
    }
   ],
   "source": [
    "log_step1 = logprob_T(theta,Arcs,Segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "90b514d5-511f-4436-ba36-3fd773a6ca14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s A\n",
      "0\n",
      "s B\n",
      "1\n",
      "s C\n",
      "2\n",
      "s D\n",
      "3\n",
      "s E\n",
      "4\n",
      "s F\n",
      "5\n",
      "s G\n",
      "6\n",
      "s H\n",
      "7\n",
      "s I\n",
      "8\n",
      "s J\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#logprob_Ts, reads, S\n",
    "#log_step2 = assign_counts(log_step1,read_counts,Segments)\n",
    "\n",
    "#log_step1.index('A')\n",
    "#read_counts[Segments.index('A')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40415f81-adcf-48c1-8466-317eb404a7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "id": "aefd7936-dd3a-4b3c-8660-fa9c5d0e20b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94418.58521193822,\n",
       " 325121.41229417035,\n",
       " 99585.98385676125,\n",
       " 119638.46661803014,\n",
       " 27891.055153880494,\n",
       " 83803.37110737103,\n",
       " 3591.414938186735,\n",
       " 88606.30524181349,\n",
       " 98514.79713156736,\n",
       " 58828.60844628089]"
      ]
     },
     "execution_count": 1077,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "id": "11e8a50d-fef1-4274-b1b2-587193cbad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expectation2(theta,read_counts,Arcs):\n",
    "    #read_counts should be a dictionar of counts\n",
    "    #theta should be an array that sums up to 1 \n",
    "    #Arcs should be an array of the Arc Transcripts ex [\"ABCD\",\"BC\"...etc]\n",
    "    \n",
    "    #first let's create the segmetns from the keys of the read_counts dictionary\n",
    "    Segments = list(read_counts.keys())\n",
    "    Arc_lengths = [len(a) for a in Arcs]\n",
    "      \n",
    "\n",
    "    #let's creata theta dictionary from the theta array and the Segments list\n",
    "    theta_dict = {}\n",
    "    for a in Arcs:\n",
    "        theta_dict[a] = theta[Arcs.index(a)]\n",
    "    prob_dict = {}\n",
    "    prob_singleRead_array = []\n",
    "    joint_prob = getSegmentProbDict(theta,Segments,Arcs)\n",
    "    sProbs = getSumProbArray(joint_prob)\n",
    "    for s in Segments:\n",
    "        prob_singleRead_array = []\n",
    "        for t in Arcs:\n",
    "            if s in t:\n",
    "                num = np.log(theta_dict[t] * (1/Arc_lengths[Arcs.index(t)]))\n",
    "                denum = sProbs[s]\n",
    "                prob_singleRead_array.append(num-denum)\n",
    "            else:\n",
    "                prob_singleRead_array.append(0)\n",
    "       # prob_singleRead_array = np.array(prob_singleRead_array)       \n",
    "#        prob_singleRead_array = prob_singleRead_array/prob_singleRead_array.sum()\n",
    "        prob_dict[s] = np.array(prob_singleRead_array)\n",
    "    #Now we have the probablity dictionary matrix of one read. posterior probablity matrix, lets multiple the reads\n",
    "    prob_dict_sums = {}\n",
    "    counts = np.zeros(len(prob_dict.keys()))\n",
    "    for s in prob_dict:\n",
    "        for i in range(len(prob_dict[s])):\n",
    "            if s in Arcs[i]:\n",
    "                 counts[i] += (np.exp(prob_dict[s][i]) * read_counts[s])\n",
    "    return counts\n",
    "#        prob_dict_sums[s] = np.sum(np.exp(prob_dict[s]) * read_counts[s])\n",
    "#print(prob_dict[s] * read_counts[s])\n",
    "#    total = sum(counts)\n",
    "    \n",
    "#    for i in range(len(counts)):\n",
    "#        counts[i] = counts[i]/total\n",
    "#    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "id": "f0982fe3-dda5-4991-8b42-1fef59aa5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testExpect = Expectation2(theta,read_counts,Arcs)\n",
    "#print(testExpect['B'])\n",
    "#print(log_step1['B'])\n",
    "#log_step1 = logprob_T(theta,Arcs,Segments)\n",
    "#testarray = [5,3,5]\n",
    "#np.exp(testarray)\n",
    "#np.exp(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e39cef-0b3e-4ada-b774-43327cd6e5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc45d0-b6cc-4f26-a1a5-57656f910bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab91a1-daf3-4942-bc65-2d7d910c0f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "id": "1eb26abf-fbf5-4966-b5a8-c50ae2e6755d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 94418.58521194, 325121.41229417,  99585.98385676, 119638.46661803,\n",
       "        27891.05515388,  83803.37110737,   3591.41493819,  88606.30524181,\n",
       "        98514.79713157,  58828.60844628])"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testExpect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "id": "9960efb8-27bc-4ccd-8be5-d37dfd5a772e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94418.58521193822,\n",
       " 325121.41229417035,\n",
       " 99585.98385676125,\n",
       " 119638.46661803014,\n",
       " 27891.055153880494,\n",
       " 83803.37110737103,\n",
       " 3591.414938186735,\n",
       " 88606.30524181349,\n",
       " 98514.79713156736,\n",
       " 58828.60844628089]"
      ]
     },
     "execution_count": 1034,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "id": "093e5e9f-636e-4584-9c6b-3feea12a3202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.exp(logprob_Ts[s][t]) 0.2200217372267056\n",
      "np.exp(logprob_Ts[s][t]) 0.6069028795343667\n",
      "np.exp(logprob_Ts[s][t]) 0.17307538323892796\n",
      "np.exp(logprob_Ts[s][t]) 0.15588527506618216\n",
      "np.exp(logprob_Ts[s][t]) 0.7214909148346109\n",
      "np.exp(logprob_Ts[s][t]) 0.12262381009920703\n",
      "np.exp(logprob_Ts[s][t]) 0.11377672570370621\n",
      "np.exp(logprob_Ts[s][t]) 0.5265979989450715\n",
      "np.exp(logprob_Ts[s][t]) 0.3596252753512221\n",
      "np.exp(logprob_Ts[s][t]) 0.03546464335316357\n",
      "np.exp(logprob_Ts[s][t]) 0.11209658260273596\n",
      "np.exp(logprob_Ts[s][t]) 0.8524387740441005\n",
      "np.exp(logprob_Ts[s][t]) 0.040951547978200095\n",
      "np.exp(logprob_Ts[s][t]) 0.31141616045031006\n",
      "np.exp(logprob_Ts[s][t]) 0.1104379491739671\n",
      "np.exp(logprob_Ts[s][t]) 0.5371943423975228\n",
      "np.exp(logprob_Ts[s][t]) 0.7382081941258597\n",
      "np.exp(logprob_Ts[s][t]) 0.26179180587414036\n",
      "np.exp(logprob_Ts[s][t]) 0.31504540233895034\n",
      "np.exp(logprob_Ts[s][t]) 0.11172499230833147\n",
      "np.exp(logprob_Ts[s][t]) 0.5434548017357619\n",
      "np.exp(logprob_Ts[s][t]) 0.029774803616956416\n",
      "np.exp(logprob_Ts[s][t]) 0.1131442922160222\n",
      "np.exp(logprob_Ts[s][t]) 0.5503585869498077\n",
      "np.exp(logprob_Ts[s][t]) 0.030153048225901487\n",
      "np.exp(logprob_Ts[s][t]) 0.30634407260826857\n",
      "np.exp(logprob_Ts[s][t]) 0.8932418470179683\n",
      "np.exp(logprob_Ts[s][t]) 0.10675815298203162\n",
      "np.exp(logprob_Ts[s][t]) 0.7781022991287728\n",
      "np.exp(logprob_Ts[s][t]) 0.22189770087122712\n"
     ]
    }
   ],
   "source": [
    "#segments\n",
    "log_step2 = assign_counts(log_step1,read_counts,Segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "a4102d30-52d7-4306-b767-3a1ff0c1cf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94418.58521193822,\n",
       " 325121.41229417035,\n",
       " 99585.98385676125,\n",
       " 119638.46661803014,\n",
       " 27891.055153880494,\n",
       " 83803.37110737103,\n",
       " 3591.414938186735,\n",
       " 88606.30524181349,\n",
       " 98514.79713156736,\n",
       " 58828.60844628089]"
      ]
     },
     "execution_count": 973,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "id": "4a63e315-8ae8-49d0-b6f7-21a125c76cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_counts(logprob_Ts, reads, S):\n",
    "    counts = [0] * 10\n",
    "    \n",
    "    for s in logprob_Ts:\n",
    "        read_s = reads[s]\n",
    "        for t in range(0, len(logprob_Ts[s])):\n",
    "            if logprob_Ts[s][t] != 0:\n",
    "                print('np.exp(logprob_Ts[s][t])',np.exp(logprob_Ts[s][t]))\n",
    "                counts[t] += read_s * np.exp(logprob_Ts[s][t])\n",
    "                \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "id": "0439d5e6-a79d-4148-b840-14c338a82114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mpresley 5/12/23, attached is the maxmize step\n",
    "def Maxmization2(counts):\n",
    "    #postprobs is a dictionary\n",
    "    # The 1st step is to convert the postprobs dictionary into a panda dataframe\n",
    "    total = sum(counts)\n",
    "    \n",
    "    for i in range(len(counts)):\n",
    "        counts[i] = counts[i]/total\n",
    "    return counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
